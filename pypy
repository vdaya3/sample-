import os
import shutil
import sys
from autogen import AssistantAgent
from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent

# ================= CONFIG =================
OPENAI_API_KEY = "your-api-key"
DOCS_PATH = "./docs"
CHROMA_PATH = "./chroma"

# ============== RE-INGEST FLAG ============
if "--reingest" in sys.argv:
    if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
        print("Chroma DB cleared. Fresh ingestion will happen on first query.")

# ============== LLM CONFIG =================
llm_config = {
    "model": "gpt-4o-mini",
    "api_key": OPENAI_API_KEY,
    "temperature": 0,
}

assistant = AssistantAgent(
    name="assistant",
    llm_config=llm_config,
    system_message=(
        "You are a RAG assistant. "
        "Answer ONLY from retrieved context. "
        "If answer not found, respond: "
        "'Not found in source documents.'"
    ),
)

ragproxy = RetrieveUserProxyAgent(
    name="ragproxy",
    human_input_mode="NEVER",
    retrieve_config={
        "task": "qa",
        "docs_path": DOCS_PATH,
        "chunk_token_size": 1000,
        "model": llm_config["model"],
        "embedding_model": "all-MiniLM-L6-v2",
        "vector_db": "chroma",
        "persist_path": CHROMA_PATH,
        "collection_name": "rag_collection",
        "get_or_create": True,
    },
)

# ============== CLI LOOP ===================
print("\n=== RAG CLI Agent ===")
print("Type 'exit' to quit\n")

while True:
    user_input = input("You: ")

    if user_input.lower() in ["exit", "quit"]:
        print("Goodbye.")
        break

    ragproxy.initiate_chat(
        assistant,
        message=user_input,
        clear_history=False,   # keeps conversation memory
    )